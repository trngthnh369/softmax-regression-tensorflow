{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16024ee8",
   "metadata": {},
   "source": [
    "# Softmax Regression with TensorFlow - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the implementation and training of Softmax Regression using TensorFlow for MNIST digit classification.\n",
    "\n",
    "## Features:\n",
    "- Data preprocessing and visualization\n",
    "- Model creation and compilation\n",
    "- Training with monitoring\n",
    "- Evaluation and analysis\n",
    "- Custom training loop demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bafd37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.model.softmax_regression import SoftmaxRegression\n",
    "from src.data.data_preprocessing import load_and_preprocess_mnist\n",
    "from src.training.trainer import ModelTrainer\n",
    "from src.training.custom_trainer import CustomTrainer\n",
    "from src.utils.visualization import *\n",
    "from src.utils.evaluation import *\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095eb24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess MNIST data\n",
    "(X_train, Y_train), (X_val, Y_val) = load_and_preprocess_mnist()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Input features: {X_train.shape[1]}\")\n",
    "print(f\"Output classes: {Y_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54609f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    \n",
    "    # Reshape back to 28x28 for display\n",
    "    image = X_train[i].numpy().reshape(28, 28)\n",
    "    true_label = tf.argmax(Y_train[i]).numpy()\n",
    "    \n",
    "    axes[row, col].imshow(image, cmap='gray')\n",
    "    axes[row, col].set_title(f'Label: {true_label}')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193674d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model_builder = SoftmaxRegression(input_size=784, num_classes=10)\n",
    "model = model_builder.create_and_compile(\n",
    "    learning_rate=0.01,\n",
    "    optimizer='sgd'\n",
    ")\n",
    "\n",
    "model_builder.get_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5a301",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer = ModelTrainer(model)\n",
    "\n",
    "history = trainer.train(\n",
    "    X_train, Y_train,\n",
    "    X_val, Y_val,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39bed3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history.history, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf63743",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "val_loss, val_accuracy = trainer.evaluate(X_val, Y_val)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Sample predictions\n",
    "Y_val_classes = tf.argmax(Y_val, axis=1).numpy()\n",
    "results = trainer.predict_samples_with_confidence(\n",
    "    X_val[:10], Y_val_classes[:10], num_samples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491ddf7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate custom training loop\n",
    "custom_model = SoftmaxRegression(784, 10).create_model()\n",
    "custom_trainer = CustomTrainer(custom_model)\n",
    "\n",
    "print(\"Training with custom loop...\")\n",
    "custom_history = custom_trainer.train(\n",
    "    X_train[:1000], Y_train[:1000],\n",
    "    X_val[:200], Y_val[:200],\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history['accuracy'][:5], label='Standard Training')\n",
    "ax1.plot(custom_history['accuracy'], label='Custom Training')\n",
    "ax1.set_title('Training Accuracy Comparison')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['loss'][:5], label='Standard Training')\n",
    "ax2.plot(custom_history['loss'], label='Custom Training')\n",
    "ax2.set_title('Training Loss Comparison')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6b4a4",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **Model Performance**: The Softmax Regression achieves ~90% accuracy on MNIST\n",
    "2. **Training Speed**: Converges quickly due to the simplicity of the model\n",
    "3. **Custom vs Standard Training**: Both approaches yield similar results\n",
    "4. **Gradient Flow**: No vanishing gradient problems for this simple model\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different optimizers (Adam, RMSprop)\n",
    "- Experiment with learning rate scheduling\n",
    "- Add regularization techniques\n",
    "- Compare with more complex models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
